{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 3-2. DDPG for Traffic Light Control\n",
    "\n",
    "Please write down the codes for DDOG algorithm on this file. Also, add your comment with the result here,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DDPG ###\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "class ReplayMemory:\n",
    "    \n",
    "    def __init__(self,length):\n",
    "        self.memory=deque(maxlen=length)\n",
    "        \n",
    "    def push(self, transition):\n",
    "        self.memory.append(transition)\n",
    "        \n",
    "    def sample(self, size):\n",
    "        sample= random.sample(self.memory, size)\n",
    "        \n",
    "        before_state=[i[0] for i in sample]\n",
    "        action=[i[1] for i in sample]\n",
    "        reward=[i[2] for i in sample]\n",
    "        state=[i[3] for i in sample]\n",
    "        done=[i[4] for i in sample]\n",
    "        \n",
    "        before_state=np.stack(before_state)\n",
    "        before_state=torch.tensor(before_state).squeeze()\n",
    "        ## print(before_state.shape)\n",
    "        \n",
    "        action=np.array(action)\n",
    "        action=torch.tensor(action, dtype=torch.float32).squeeze(-1)\n",
    "        \n",
    "        ##print(action.shape)\n",
    "        state=np.stack(state)\n",
    "        state=torch.tensor(state).squeeze()\n",
    "        \n",
    "        reward=np.array(reward)\n",
    "        reward=torch.tensor(reward,dtype=torch.float32).reshape(-1,1)\n",
    "        \n",
    "        done = np.array(done).astype(int)\n",
    "        done=torch.tensor(done).reshape(-1,1)\n",
    "        \n",
    "        return before_state, action, reward, state, done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=[64,64]):\n",
    "        super(Actor,self).__init__()\n",
    "        self.layers=nn.ModuleList()\n",
    "        self.activations=nn.ModuleList()\n",
    "        \n",
    "        input_dims=[state_dim]+hidden_dim\n",
    "        output_dims=hidden_dim + [action_dim]\n",
    "        \n",
    "        for in_dim, out_dim in zip(input_dims, output_dims):\n",
    "            self.layers.append(nn.Linear(in_dim, out_dim))\n",
    "            \n",
    "        for i in range(len(hidden_dim)):\n",
    "            self.activations.append(nn.LeakyReLU())\n",
    "            \n",
    "        self.activations.append(nn.Tanh())\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x=state\n",
    "        for l, activation in zip(self.layers, self.activations):\n",
    "            x=l(x)\n",
    "            x=activation(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64, output_dim=1):\n",
    "        super(Critic, self).__init__()\n",
    "        self.l1=nn.Linear(state_dim, hidden_dim)\n",
    "        self.l2=nn.Linear(hidden_dim + action_dim, hidden_dim)\n",
    "        self.l3=nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x=F.relu(self.l1(state))\n",
    "        x=F.relu(self.l2(torch.cat([x,action],dim=-1)))\n",
    "        x=self.l3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class OUNoise:\n",
    "    \n",
    "    def __init__(self, action_space, mu=0.0, theta=0.15, max_sigma=0.3, min_sigma=0.3, decay_period=1000):\n",
    "        self.mu=mu\n",
    "        self.theta=theta\n",
    "        self.sigma=max_sigma\n",
    "        self.max_sigma=max_sigma\n",
    "        self.min_sigma=min_sigma\n",
    "        self.decay_period=decay_period\n",
    "        self.action_dim=action_space\n",
    "        self.reset()\n",
    "        \n",
    "        self.epsilon=1.0\n",
    "        self.epsilon_decay=0.00001\n",
    "        self.epsilon_min=0.05\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state=np.ones(self.action_dim)*self.mu\n",
    "        \n",
    "    def evolve_state(self):\n",
    "        x=self.state\n",
    "        dx=self.theta*(self.mu-x)+self.sigma*np.random.randn(self.action_dim)\n",
    "        self.state=x+dx\n",
    "        return self.state\n",
    "    \n",
    "    def get_action(self, action,t=0):\n",
    "        ou_state=self.evolve_state()*self.epsilon\n",
    "        \n",
    "        self.epsilon-=self.epsilon_decay\n",
    "        if self.epsilon<self.epsilon_min:\n",
    "            self.epsilon=self.epsilon_min\n",
    "            \n",
    "        self.sigma=self.max_sigma-(self.max_sigma-self.min_sigma) * min(1.0, t/ self.decay_period)\n",
    "        return np.clip(action+ou_state,-1.0,1.0)\n",
    "\n",
    "class DDPGAgent(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, action_min, action_max, gamma=0.99):\n",
    "        super(DDPGAgent,self).__init__()\n",
    "        self.action_min=np.array(action_min)\n",
    "        self.action_max=np.array(action_max)\n",
    "        \n",
    "        self.gamma=gamma\n",
    "        self.ou_noise=OUNoise(action_dim)\n",
    "        \n",
    "        self.actor=Actor(state_dim, action_dim)\n",
    "        self.critic=Critic(state_dim,action_dim)\n",
    "        \n",
    "        self.actor_target=Actor(state_dim,action_dim)\n",
    "        self.critic_target=Critic(state_dim, action_dim)\n",
    "        \n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        \n",
    "        self.actor_optimizer=torch.optim.Adam(self.actor.parameters(), lr=1e-4)\n",
    "        self.critic_optimizer=torch.optim.Adam(self.critic.parameters(), lr=1e-3)\n",
    "        \n",
    "        self.memory=ReplayMemory(5000)\n",
    "        self.batch_size=50\n",
    "        \n",
    "        self.num_fit=0\n",
    "        \n",
    "        self.loss_ftn=nn.MSELoss()\n",
    "        \n",
    "    def train_start(self):\n",
    "        \n",
    "        if len(self.memory) > self.batch_size:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def forward(self, state, t=0):\n",
    "        action_before_norm=self.actor(state).detach().numpy()\n",
    "        action_before_norm_with_noise = self.ou_noise.get_action(action_before_norm, t)\n",
    "        action_after_norm=(action_before_norm_with_noise + 1) / 2 * (self.action_max-self.action_min)+self.action_min\n",
    "        \n",
    "        return action_before_norm, action_after_norm\n",
    "    \n",
    "    def save_memory(self, transition):\n",
    "        self.memory.push(transition)\n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        state, action, reward, next_state, done= self.memory.sample(self.batch_size)\n",
    "        \n",
    "        q=self.critic(state,action)\n",
    "        \n",
    "        next_q_val=self.critic_target(next_state,self.actor_target(next_state))\n",
    "        target_q=reward+self.gamma*(1-done)*next_q_val\n",
    "        \n",
    "        critic_loss=self.loss_ftn(q,target_q)\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        actor_loss=-self.critic(state,self.actor(state))\n",
    "        actor_loss=actor_loss.mean()\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        if self.num_fit % 100==0:\n",
    "            self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "            self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "            \n",
    "        return critic_loss.item(), actor_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -112\n",
      "2 -168\n",
      "3 -278\n",
      "4 -703\n",
      "5 -564\n",
      "6 -118\n",
      "7 -93\n",
      "8 -111\n",
      "9 -110\n",
      "10 -124\n",
      "11 -107\n",
      "12 -93\n",
      "13 -96\n",
      "14 -116\n",
      "15 -119\n",
      "16 -97\n",
      "17 -90\n",
      "18 -98\n",
      "19 -115\n",
      "20 -111\n",
      "21 -107\n",
      "22 -114\n",
      "23 -99\n",
      "24 -103\n",
      "25 -110\n",
      "26 -97\n",
      "27 -129\n",
      "28 -96\n",
      "29 -126\n",
      "30 -120\n",
      "31 -102\n",
      "32 -109\n",
      "33 -90\n",
      "34 -100\n",
      "35 -92\n",
      "36 -100\n",
      "37 -110\n",
      "38 -105\n",
      "39 -113\n",
      "40 -93\n",
      "41 -109\n",
      "42 -107\n",
      "43 -111\n",
      "44 -97\n",
      "45 -92\n",
      "46 -112\n",
      "47 -92\n",
      "48 -152\n",
      "49 -99\n",
      "50 -129\n",
      "51 -121\n",
      "52 -108\n",
      "53 -114\n",
      "54 -92\n",
      "55 -103\n",
      "56 -110\n",
      "57 -195\n",
      "58 -104\n",
      "59 -99\n",
      "60 -95\n",
      "61 -110\n",
      "62 -111\n",
      "63 -89\n",
      "64 -95\n",
      "65 -90\n",
      "66 -115\n",
      "67 -117\n",
      "68 -92\n",
      "69 -118\n",
      "70 -113\n",
      "71 -98\n",
      "72 -103\n",
      "73 -105\n",
      "74 -106\n",
      "75 -163\n",
      "76 -104\n",
      "77 -94\n",
      "78 -95\n",
      "79 -95\n",
      "80 -94\n",
      "81 -100\n",
      "82 -114\n",
      "83 -121\n",
      "84 -110\n",
      "85 -89\n",
      "86 -92\n",
      "87 -112\n",
      "88 -106\n",
      "89 -118\n",
      "90 -93\n",
      "91 -108\n",
      "92 -94\n",
      "93 -108\n",
      "94 -93\n",
      "95 -92\n",
      "96 -87\n",
      "97 -103\n",
      "98 -107\n",
      "99 -124\n",
      "100 -114\n",
      "101 -110\n",
      "102 -111\n",
      "103 -109\n",
      "104 -99\n",
      "105 -96\n",
      "106 -103\n",
      "107 -93\n",
      "108 -107\n",
      "109 -100\n",
      "110 -120\n",
      "111 -98\n",
      "112 -105\n",
      "113 -93\n",
      "114 -97\n",
      "115 -109\n",
      "116 -110\n",
      "117 -118\n",
      "118 -111\n",
      "119 -110\n",
      "120 -92\n",
      "121 -119\n",
      "122 -116\n",
      "123 -119\n",
      "124 -110\n",
      "125 -99\n",
      "126 -108\n",
      "127 -118\n",
      "128 -97\n",
      "129 -112\n",
      "130 -100\n",
      "131 -108\n",
      "132 -121\n",
      "133 -118\n",
      "134 -96\n",
      "135 -105\n",
      "136 -97\n",
      "137 -98\n",
      "138 -101\n",
      "139 -127\n",
      "140 -124\n",
      "141 -108\n",
      "142 -105\n",
      "143 -96\n",
      "144 -128\n",
      "145 -93\n",
      "146 -95\n",
      "147 -114\n",
      "148 -93\n",
      "149 -103\n",
      "150 -95\n",
      "151 -112\n",
      "152 -110\n",
      "153 -109\n",
      "154 -100\n",
      "155 -91\n",
      "156 -102\n",
      "157 -116\n",
      "158 -115\n",
      "159 -111\n",
      "160 -118\n",
      "161 -91\n",
      "162 -124\n",
      "163 -106\n",
      "164 -105\n",
      "165 -106\n",
      "166 -98\n",
      "167 -89\n",
      "168 -95\n",
      "169 -91\n",
      "170 -116\n",
      "171 -98\n",
      "172 -114\n",
      "173 -92\n",
      "174 -139\n",
      "175 -91\n",
      "176 -121\n",
      "177 -115\n",
      "178 -100\n",
      "179 -91\n",
      "180 -103\n",
      "181 -113\n",
      "182 -90\n",
      "183 -102\n",
      "184 -93\n",
      "185 -101\n",
      "186 -97\n",
      "187 -105\n",
      "188 -110\n",
      "189 -102\n",
      "190 -117\n",
      "191 -101\n",
      "192 -99\n",
      "193 -91\n",
      "194 -98\n",
      "195 -119\n",
      "196 -91\n",
      "197 -102\n",
      "198 -113\n",
      "199 -112\n",
      "200 -100\n",
      "201 -123\n",
      "202 -101\n",
      "203 -94\n",
      "204 -120\n",
      "205 -94\n",
      "206 -101\n",
      "207 -105\n",
      "208 -114\n",
      "209 -98\n",
      "210 -95\n",
      "211 -96\n",
      "212 -96\n",
      "213 -94\n",
      "214 -105\n",
      "215 -117\n",
      "216 -118\n",
      "217 -122\n",
      "218 -107\n",
      "219 -91\n",
      "220 -126\n",
      "221 -93\n",
      "222 -133\n",
      "223 -100\n",
      "224 -99\n",
      "225 -92\n",
      "226 -121\n",
      "227 -111\n",
      "228 -94\n",
      "229 -96\n",
      "230 -127\n",
      "231 -106\n",
      "232 -101\n",
      "233 -125\n",
      "234 -101\n",
      "235 -109\n",
      "236 -122\n",
      "237 -117\n",
      "238 -92\n",
      "239 -124\n",
      "240 -91\n",
      "241 -107\n",
      "242 -90\n",
      "243 -105\n",
      "244 -88\n",
      "245 -94\n",
      "246 -111\n",
      "247 -106\n",
      "248 -110\n",
      "249 -96\n",
      "250 -100\n",
      "251 -100\n",
      "252 -122\n",
      "253 -94\n",
      "254 -118\n",
      "255 -108\n",
      "256 -96\n",
      "257 -102\n",
      "258 -90\n",
      "259 -102\n",
      "260 -90\n",
      "261 -113\n",
      "262 -116\n",
      "263 -108\n",
      "264 -115\n",
      "265 -89\n",
      "266 -100\n",
      "267 -129\n",
      "268 -90\n",
      "269 -122\n",
      "270 -96\n",
      "271 -116\n",
      "272 -108\n",
      "273 -106\n",
      "274 -102\n",
      "275 -91\n",
      "276 -108\n",
      "277 -109\n",
      "278 -101\n",
      "279 -114\n",
      "280 -105\n",
      "281 -92\n",
      "282 -124\n",
      "283 -95\n",
      "284 -118\n",
      "285 -92\n",
      "286 -88\n",
      "287 -103\n",
      "288 -100\n",
      "289 -116\n",
      "290 -93\n",
      "291 -88\n",
      "292 -109\n",
      "293 -111\n",
      "294 -111\n",
      "295 -108\n",
      "296 -113\n",
      "297 -113\n",
      "298 -104\n",
      "299 -107\n",
      "300 -106\n"
     ]
    }
   ],
   "source": [
    "### Training (Do not revise this code)###\n",
    "from SUMO.TrafficEnv_DDPG import TrafficEnv\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def main_DDPG() :\n",
    "\n",
    "    exp_dir = './SUMO/Single_Intersection'\n",
    "    exp_type = 'binary'\n",
    "\n",
    "    max_episode = 300\n",
    "    max_epi_step = 800\n",
    "\n",
    "    state_dim = 12\n",
    "    action_dim = 1\n",
    "\n",
    "    action_min = 5\n",
    "    action_max = 20\n",
    "\n",
    "    env = TrafficEnv(exp_dir, exp_type)\n",
    "    agent = DDPGAgent(state_dim, action_dim, action_min, action_max)\n",
    "\n",
    "    actor_loss_list = []\n",
    "    critic_loss_list = []\n",
    "    reward_list = []\n",
    "\n",
    "    for episode in range(max_episode):\n",
    "\n",
    "        state = env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.float32).reshape(-1, state_dim)\n",
    "\n",
    "        actor_loss_epi = []\n",
    "        critic_loss_epi = []\n",
    "        reward_epi = []\n",
    "        timing_data_epi = []\n",
    "        action = None\n",
    "        step = 0\n",
    "        current_phase = 0\n",
    "\n",
    "        for epi_step in range(max_epi_step):\n",
    "\n",
    "            # make an action based on epsilon greedy action\n",
    "            before_action = action\n",
    "\n",
    "            action, action_norm = agent.forward(state, step)\n",
    "\n",
    "            before_state = state\n",
    "\n",
    "            state, reward, done = env.step(action_norm)\n",
    "\n",
    "            state = torch.tensor(state, dtype=torch.float32).reshape(-1, state_dim)\n",
    "            reward_epi.append(reward)\n",
    "\n",
    "            # make a transition and save to replay memory\n",
    "            transition = [before_state, action, reward, state, done]\n",
    "            agent.save_memory(transition)\n",
    "\n",
    "            if agent.train_start():\n",
    "                critic_loss, actor_loss = agent.train()\n",
    "\n",
    "                critic_loss_epi.append(critic_loss)\n",
    "                actor_loss_epi.append(actor_loss)\n",
    "\n",
    "            if done:\n",
    "                if agent.train_start():\n",
    "                    critic_mean = sum(critic_loss_epi) / len(critic_loss_epi)\n",
    "                    actor_mean = sum(actor_loss_epi) / len(actor_loss_epi)\n",
    "                    \n",
    "                    critic_loss_list.append(critic_mean)\n",
    "                    actor_loss_list.append(actor_mean)\n",
    "                break\n",
    "\n",
    "            step += 1\n",
    "\n",
    "        reward_list.append(sum(reward_epi))  \n",
    "    \n",
    "        env.close()\n",
    "\n",
    "        print(episode+1, reward_list[-1])\n",
    "\n",
    "    return critic_loss_list, actor_loss_list, reward_list\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    critic_loss_list, actor_loss_list, reward_list = main_DDPG()\n",
    "\n",
    "    plt.plot(critic_loss_list)\n",
    "    plt.title(\"Critic Loss of DDPG Agent\")\n",
    "    plt.savefig(\"./Visualization/DDPG/critic_loss.png\")\n",
    "    plt.close('all')\n",
    "    \n",
    "    plt.plot(actor_loss_list)\n",
    "    plt.title(\"Actor Loss of DDPG Agent\")\n",
    "    plt.savefig(\"./Visualization/DDPG/actor_loss.png\")\n",
    "    plt.close('all')\n",
    "\n",
    "    plt.plot(reward_list)\n",
    "    plt.title(\"Reward of DDPG Agent\")\n",
    "    plt.savefig(\"./Visualization/DDPG/reward.png\")\n",
    "    plt.close('all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add your comment for the result here ###\n",
    "step을 진행할수록 reward가 fluctuate하긴 하지만, -100 정도로 유지됨을 볼 수 있다.\n",
    "hyperparameter를 바꿔가며 모델을 발전시킨 것은 아니고, train의 결과이지만,\n",
    "DQN의 방법에서 나온 reward보다 훨씬 좋게 나왔다.\n",
    "actor loss 와 critic loss 는 점점 줄어드는 것을 볼 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
